--- 
layout: post
title: The pitfalls of building services using Google App Engine
date: 2013-02-24 08:18:17
author: usman
categories: 
- System Admin
tags:
- GAE

---

A lot has been written about the problems of working with [Google App Engine's](https://appengine.google.com/) for example [here](http://www.carlosble.com/2010/11/goodbye-google-app-engine-gae/), [here](http://3.14.by/en/read/why-google-appengine-sucks) and [here](http://www.zdnet.com/blog/google/the-problem-with-google-apps-engine/1002). The majority of the complaints are from people who used the platform experimentally for a few days, were frustrated by its intricacies and deviation from the typical LAMP and RDBMS paradigms. They wrote angry blogs and went back to their old platforms. The problem is that these complaints are easy to dismiss and hide some real issues in Google App Engine. We on the other hand did our due diligence and then some. We launched a back-end service for the [Simpsons Tapped Out](https://play.google.com/store/apps/details?id=com.ea.game.simpsons4_na) to production and have a hundred thousand concurrent users. We have used pretty much every service GAE has to offer from Data Store to Big Query to the Identity API. Below are the major hidden pitfalls you should know before using google app engine to author any service of non-trivial scale or complexity.  

## Hidden Arbitrary Quota Limits

After a few months of hard work we soft launched our application in a few countries and saw traffic gradually pick up. No surprises, we all patted ourselves on the back and did our global launch. The service performed as expected and we were happily chugging along. Part of that chugging was using the GAE App Identity service to sign authorization tokens. 
One fine day we found out that all our logins were failing, apparently there is a quota limit of 1.84 millions requests per day to the App Identity service. After which google "helps you control your costs" by refusing all your requests to the API for the day. Let me step back and let that sink in. We have a paid account with google on the highest tier of service and fully expected millions of requests per day and are willing to pay for them. Google decided that we should save money by denying all requests for the remainder of the day. Now just to clarify its not that we weren't watching our quota closely enough, this quota of calls to the Identify API is not shown on the app stats page (or any page for that matter). None of the documentation mentions that there is a hard limit of 1.84 million requests to the API, and at no point over the past days when our call count was in the the many hundreds of thousands of requests did someone at google decide to alert us that our app would suddenly stop working soon.   

## Under-powered instances

The reason we were using GAE's APP Identity API for signing was that google instances are are greatly under-powered. This is not always obvious because most requests are not very CPU intensive and google launches a whole lot of servers so each one is rarely handling more that a few requests at a time. However, we were using RSA cryptography to sign our authorization tokens. I grant you that this is a CPU intensive task and we are willing to take the performance hit. However a quick benchmark showed that the signature generation takes on average 10ms on my rather modest laptop. The same code on a google app engine server under no load takes 400-600ms. This time does not include network latency or any scheduler overhead, this is just the time time taken to compute the signature.   


## Hidden Arbitrary Library behaviour 

While we are on the subject of Auth tokens, we noticed another strange issue while building the token verification service. We send our Auth token as a header with all requests so that various services check user authenticity and authorization. The fact that this token is in the header makes it easy for us to use a servlet filter and check authorization before any business logic code is hit and data is read from the request. What could be simpler right? Well what if every so often the header disappeared, no warning no error the header just magically disappeared. We realized that if any Http Header happens to be longer than a certain length (approximately 400 bytes) the google http client will silently drop the header and issue the http request with out it. Now I understand some proxies mangle headers and browsers don't like big headers so google may want to dissuade us from using big headers. 400 bytes seems somewhat low as a hard limit but lets say I agree that it is reasonable. How would it even be reasonable to silently drop the header on the client side and send the request anyway. Perhaps a warning in the client log that the header was too large and dropped would be useful. If the programmer explicitly set a header its a fair bet the server needs the information in that header. Why issue the request anyway without the header? Wouldn't it have been more clear just to throw an exception and tell the client developer that big headers are not allowed? That would be too easy, instead we had to spend days of trial and error to figure out exactly why the requests were getting mangled. 
 
## No pre-scaling mechanism

As any one who has launched a large scale service knows, deployments to production are tricky. Deployments under load are more so that is why most deployments happen in off-peak hours. However for us even our deepest trough has a few tens of thousand active users and we want to do hot deploys. With EC2 we would bring up a copy of the prod environment and point the load balancer at the new environments while keeping the old one running for connected clients. Technically it should have been even simpler with GAE, you deploy a new version of your server and make the new version default. The problem is that GAE is really bad at scaling rapidly for load. It takes up to a lot of time for the GAE scheduler to detect that response times are rising and instances need to be launched. During this time a lot of your clients will get time out errors and very high response times. We can side step this problem by using residence instances, i.e. if we know we are going to be getting a lot of traffic we can pre-launch instances. This is all great, full marks for App engine but the one small detail that completely negates all of these features; you can only launch resident instances for the default version of the app.  Why google Why? This means that when we are doing code pushes and we deploy to the new version of the app there is no way to pre-launch instances for the new version. When we make the new version it will have exactly 1 instance. Even at minimum load our services needs about 30 instances. If we ever had to do a critical fix under load when our app is running at about 200 instances we would be have massive client disruption. We contacted google about this issue and the official suggestion was to run a load test against the prod environment with dummy users to trick the scheduler into running more instances and then quickly shut down the load test and flick the switch to send real users to the new version. 

## Disconnect between services

Another design decision that has me pulling out my hair the complete disconnect between the various google services. This was highlighted when we needed to get data from our DataStore into Big Query for analysis. To get data into BigQuery you either need to upload it from your local machine or first copy it into Cloud Storage. While this is fine for manual operation but we are generating many gigabytes of data daily. Why is it necessary to copy it all to local storage and incur the network bandwidth cost and then load back into BigQuery and take that cost hit again. Data can be imported into BigQuery in two formats, CSV and JSON however data can only be exported in CSV format. We use a lot of JSON hence its very easy for us to write JSON to be ingested by BigQuery and the BigQuery certainly speaks JSON as it allows import of this format. Why is is it arbitrarily denying export to JSON? Google has a detailed ACL mechanism to manage access to cloud storage however in order to authorize a user data access to BigQuery there is a completely separate authentication mechanism. Oh how I yearn for IAMs service on amazon. The ACLs on the cloud storage bucket can only be set using the appcfg.py tool which has to be run locally, the cloud storage access can only be setup using the API console. I could go on, it seams like no two services were designed to work with each other cleanly. 

## User Interface for Androids

![Backup](/assets/images/backup.jpg)

The last thing I would like to highlight is the image above, for me it symbolizes what is wrong with GAE. Its a actual screen shot from a menu in GAE console for restoring backups. These backups were generated through google's standard tool and this is the User interface that allows you to select which up to restore from. The fact that some google developer somewhere wrote this UI looked at it and said "Sure, looks good you can select the backup you want lets ship it" scares me. It shows either an utter disregard for the usability of the server or more likely someone wrote the backup service which used random hashes as file names and assumed the restore code will look in the file for metadata and someone else wrote the restore code and assumed the file names would be human readable. Neither of these cases give me confidence that whats "under the hood" of google app engine has been thought through very well. This is important because you as a service developer are relying on GAE to have good plumbing under the hood as you have no control over it. If I had to write our service again I would probably go with Amazon Elastic Beanstalk or something less. For now it all kind of works and we are too far down this path so we will keep our fingers crossed. 



