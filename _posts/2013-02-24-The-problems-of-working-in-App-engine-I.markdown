--- 
layout: post
title: The pitfalls of building services using Google App Engine -- Part I
date: 2013-02-24 08:18:17
author: usman
categories: 
- System Admin
tags:
- GAE

---

A lot has been written about the problems of working with [Google App Engine's](https://appengine.google.com/) for example [here](http://www.carlosble.com/2010/11/goodbye-google-app-engine-gae/), [here](http://3.14.by/en/read/why-google-appengine-sucks) and [here](http://www.zdnet.com/blog/google/the-problem-with-google-apps-engine/1002). The majority of the complaints are from people who used the platform experimentally for a few days, were frustrated by its intricacies and deviation from the typical LAMP and RDBMS paradigms. They wrote angry blogs and went back to their old platforms. The problem is that these complaints are easy to dismiss and hide some real issues in Google App Engine. We on the other hand did our due diligence and then some. We launched a back-end service for the [Simpsons Tapped Out](https://play.google.com/store/apps/details?id=com.ea.game.simpsons4_na) to production and have a hundred thousand concurrent users. We have used pretty much every service GAE has to offer from Data Store to Big Query to the Identity API. We have compiled some of the major pitfalls in a two part series. The next article in the series is found [here](http://techtraits.com/2013-02-24-The-problems-of-working-in-App-engine-II.html).

## Hidden Arbitrary Quota Limits

After a few months of hard work we soft launched our application in a few countries and saw traffic gradually pick up. No surprises, we all patted ourselves on the back and did our global launch. The service performed as expected and we were happily chugging along. Part of that chugging was using the GAE App Identity service to sign authorization tokens. 
One fine day we found out that all our logins were failing, apparently there is a quota limit of 1.84 millions requests per day to the App Identity service. After which google "helps you control your costs" by refusing all your requests to the API for the day. Let me step back and let that sink in. We have a paid account with google on the highest tier of service and fully expected millions of requests per day and are willing to pay for them. Google decided that we should save money by denying all requests for the remainder of the day. Now just to clarify its not that we weren't watching our quota closely enough, this quota of calls to the Identify API is not shown on the app stats page (or any page for that matter). None of the documentation mentions that there is a hard limit of 1.84 million requests to the API, and at no point over the past days when our call count was in the many hundreds of thousands of requests did someone at google decide to alert us that our app would suddenly stop working soon.   

## Hidden Arbitrary Library behaviour 

While we are on the subject of Auth tokens, we noticed another strange issue while building the token verification service. We send our Auth token as a header with all requests so that various services check user authenticity and authorization. The fact that this token is in the header makes it easy for us to use a servlet filter and check authorization before any business logic code is hit and data is read from the request. What could be simpler right? Well what if every so often the header disappeared, no warning no error the header just magically disappeared. We realized that if any Http Header happens to be longer than a certain length (approximately 400 bytes) the google http client will silently drop the header and issue the http request with out it. Now I understand some proxies mangle headers and browsers don't like big headers so google may want to dissuade us from using big headers. 400 bytes seems somewhat low as a hard limit but lets say I agree that it is reasonable. How would it even be reasonable to silently drop the header on the client side and send the request anyway. Perhaps a warning in the client log that the header was too large and dropped would be useful. If the programmer explicitly set a header its a fair bet the server needs the information in that header. Why issue the request anyway without the header? Wouldn't it have been more clear just to throw an exception and tell the client developer that big headers are not allowed? That would be too easy, instead we had to spend days of trial and error to figure out exactly why the requests were getting mangled. 
 
## No pre-scaling mechanism

As any one who has launched a large scale service knows, deployments to production are tricky. Deployments under load are more so that is why most deployments happen in off-peak hours. However for us even our deepest trough has a few tens of thousand active users and we want to do hot deploys. With EC2 we would bring up a copy of the prod environment and point the load balancer at the new environments while keeping the old one running for connected clients. Technically it should have been even simpler with GAE, you deploy a new version of your server and make the new version default. The problem is that GAE is really bad at scaling rapidly for load. It takes up to a lot of time for the GAE scheduler to detect that response times are rising and instances need to be launched. During this time a lot of your clients will get time out errors and very high response times. We can side step this problem by using residence instances, i.e. if we know we are going to be getting a lot of traffic we can pre-launch instances. This is all great, full marks for App engine but the one small detail that completely negates all of these features; you can only launch resident instances for the default version of the app.  Why google Why? This means that when we are doing code pushes and we deploy to the new version of the app there is no way to pre-launch instances for the new version. When we make the new version it will have exactly 1 instance. Even at minimum load our services needs about 30 instances. If we ever had to do a critical fix under load when our app is running at about 200 instances we would be have massive client disruption. We contacted google about this issue and the official suggestion was to run a load test against the prod environment with dummy users to trick the scheduler into running more instances and then quickly shut down the load test and flick the switch to send real users to the new version. 

## User Interface for Androids

![Backup](/assets/images/backup.jpg)

Another thing I would like to highlight is the image above, for me it symbolizes what is wrong with GAE. Its a actual screen shot from a menu in GAE console for restoring backups. These backups were generated through google's standard tool and this is the User interface that allows you to select which up to restore from. The fact that some google developer somewhere wrote this UI looked at it and said "Sure, looks good you can select the backup you want lets ship it" scares me. It shows either an utter disregard for the usability of the server or more likely someone wrote the backup service which used random hashes as file names and assumed the restore code will look in the file for metadata and someone else wrote the restore code and assumed the file names would be human readable.

If you have made it this far and I still haven't convinced you, checkout the next part of the series: 
**[ The pitfalls of building services using Google App Engine -- Part II](http://techtraits.com/2013-02-24-The-problems-of-working-in-App-engine-II.html)**